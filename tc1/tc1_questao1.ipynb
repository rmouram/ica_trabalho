{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb718b4-2376-4582-86cd-be41c37de427",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Minimos Quadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be4d8e0d-9f14-4e6d-861d-513c9268535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "def calculate_error(X, y, theta):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Realiza as predições\n",
    "    predictions = X @ theta\n",
    "\n",
    "    # Converte as predições em rótulos\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calcula o erro (taxa de erro)\n",
    "    error = np.mean(predicted_labels != y)\n",
    "\n",
    "    return error\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71af66-3249-4d08-83e4-2d9461eac7fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Perceptron Logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2677d38-1d40-48f7-b3ea-2a2cfd6bb524",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d58910a3-6ab3-4a8d-ac22-e317f54e6fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 88.05%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Cria uma instância do perceptron logístico\n",
    "model = Perceptron()\n",
    "\n",
    "# Treina o modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realiza as predições no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcula a acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Acurácia da classificação: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce720c1-b0d8-4d7a-8ecd-c69b4bf3376f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## A mão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14754d2-4bbd-4309-9a10-fd281ca699a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6550bb67-5c48-423d-8766-4162dc1f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)\n",
    "                error = target - probabilities\n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f47de887-e728-479f-8d75-7cc8c13eacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1077f8c-da7b-4923-a3be-950fef6e1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c37ffb7-1572-431d-80c2-a5d74f9089c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68e4d936-67fa-4096-8b98-11af26455203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16d57182-1cb0-4199-8523-9b1f906e0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o perceptron\n",
    "perceptron.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfcdb8d0-1f5f-4980-bad4-e95907d9a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b647280-98d3-4bfb-8f15-2e9bcdf6a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9099\n"
     ]
    }
   ],
   "source": [
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68398f-be04-41f0-abe1-decea7f82c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Minimos Quadrados + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f61694d1-cf06-4006-8c2a-e439d25e72ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 100\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train_pca, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test_pca.shape[0], 1)), X_test_pca), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00aab16-51f0-4b90-a52f-27244626dfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dc844-91eb-4abb-9c83-7beb088322d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac75ae3-1a38-4555-995f-7f6c9c779dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b226a62-be74-4724-9c5c-e03185f29fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a539259e-17ea-4031-9f18-60a95f51fd51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,) and (784,155) not aligned: 10000 (dim 0) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m X_train_pca \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_train, principal_components)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Projetar os dados de teste nas componentes principais\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m X_test_pca \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprincipal_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# -------------------------- PCA -------------------------- #\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Converte os rótulos para uma representação one-hot\u001b[39;00m\n\u001b[1;32m     53\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10000,) and (784,155) not aligned: 10000 (dim 0) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados de treino nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados de teste nas componentes principais\n",
    "X_test_pca = np.dot(y_test, principal_components)\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train_pca, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test_pca), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e16181-2f23-43b4-b3b2-cb66c4dc334a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Perceptron Logistico + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53df544-62e6-46ce-a08c-99e47885910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 21:45:48.646604: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 21:45:49.285956: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 21:45:49.289104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 21:45:50.866213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)\n",
    "                error = target - probabilities\n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta\n",
    "\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train_pca.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)\n",
    "\n",
    "# Treinar o perceptron\n",
    "perceptron.train(X_train_pca, y_train)\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test_pca:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d43cdd-c75e-4bcb-92de-d7f23bcd698e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26ebd276-be88-4963-a255-1f106d27f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra do valor médio:  397\n",
      "Regra da raiz quadrada:  88\n",
      "Regra de kolmogorov:  1569\n"
     ]
    }
   ],
   "source": [
    "# Regras heurísticas para determinação do número de neurônios ocultos (q)\n",
    "# fonte referência: https://repositorio.ufc.br/bitstream/riufc/52214/3/2020_dis_juponte.pdf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "p = 28*28\n",
    "m = 10\n",
    "\n",
    "# Regra do valor médio\n",
    "q1 = (p + m) / 2\n",
    "q1 = int(q1)\n",
    "print('Regra do valor médio: ', q1)\n",
    "\n",
    "# Regra da raiz quadrada\n",
    "q2 = np.sqrt(p * m)\n",
    "q2 = int(q2)\n",
    "print('Regra da raiz quadrada: ', q2)\n",
    "\n",
    "# Regra de Kolmogorov\n",
    "q3 = 2*p + 1\n",
    "q3 = int(q3)\n",
    "print('Regra de kolmogorov: ', q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "634447af-479a-44b8-94d3-89350dfd455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests, gzip, os, hashlib\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "(X, Y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Validation split\n",
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val=X[train_no,:,:],X[val_no,:,:]\n",
    "y_train,y_val=Y[train_no],Y[val_no]\n",
    "\n",
    "def init(x, y):\n",
    "    layer = np.random.uniform(-1, 1., size=(x,y)) / np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1)\n",
    "\n",
    "# derivative of sigmoid\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "# def d_sigmoid(x):\n",
    "#     return (np.exp(-x)) / ((np.exp(-x) + 1)**2)\n",
    "\n",
    "# sofmax function\n",
    "def softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0)\n",
    "\n",
    "# derivative of softmax\n",
    "def d_softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0) * (1-exp_element / np.sum(exp_element, axis=0))\n",
    "\n",
    "# foward and backward pass\n",
    "def forward_backward_pass(x, y):\n",
    "    targets = np.zeros((len(y), 10), np.float32)\n",
    "    targets[range(targets.shape[0]), y] = 1    \n",
    "    \n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = softmax(x_l2)\n",
    "\n",
    "    error = 2 * (out - targets) / out.shape[0] * d_softmax(x_l2)\n",
    "    update_l2 = x_sigmoid.T @ error\n",
    "\n",
    "    error = ((l2).dot(error.T)).T * d_sigmoid(x_l1)\n",
    "    update_l1 = x.T @ error\n",
    "\n",
    "    return out, update_l1, update_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beb7f6-4b07-4f6b-a798-15338d2c54ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regra 1: Valor médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e9bd586-eae1-49c0-aff3-2cc56820b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (128, 784)\n",
      "y:  (128,)\n",
      "For 0th epoch: train accuracy: 0.047 | validation accuracy:0.080\n",
      "x:  (128, 784)\n",
      "y:  (128,)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q1)\n",
    "l2 = init(q1, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "33a993dc-79ec-481d-a5d4-6727556cd9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 65.08%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb674c-bf15-456c-992b-8ec2d3daae59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6701e6e-7812-459d-9691-d40f7a08c30e",
   "metadata": {},
   "source": [
    "## Regra 2: Raiz quadrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee1deb18-5501-4dfb-b257-cd96ae9c3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.086 | validation accuracy:0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3689/634698362.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (np.exp(-x)+1)\n",
      "/tmp/ipykernel_3689/634698362.py:29: RuntimeWarning: overflow encountered in multiply\n",
      "  return x * (1 - x)\n",
      "/tmp/ipykernel_3689/634698362.py:58: RuntimeWarning: invalid value encountered in matmul\n",
      "  update_l1 = x.T @ error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 500th epoch: train accuracy: 0.094 | validation accuracy:0.098\n",
      "For 1000th epoch: train accuracy: 0.125 | validation accuracy:0.098\n",
      "For 1500th epoch: train accuracy: 0.117 | validation accuracy:0.098\n",
      "For 2000th epoch: train accuracy: 0.125 | validation accuracy:0.098\n",
      "For 2500th epoch: train accuracy: 0.102 | validation accuracy:0.098\n",
      "For 3000th epoch: train accuracy: 0.086 | validation accuracy:0.098\n",
      "For 3500th epoch: train accuracy: 0.094 | validation accuracy:0.098\n",
      "For 4000th epoch: train accuracy: 0.133 | validation accuracy:0.098\n",
      "For 4500th epoch: train accuracy: 0.086 | validation accuracy:0.098\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q2)\n",
    "l2 = init(q2, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4fd53e1f-5c5b-407a-bb87-98d2ee0c83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 74.46%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26385606-fb63-44bb-ada8-002f02f577da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56a923c5-5a97-41cb-8f7a-914e58127876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regra 3: kolmogorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c902cbd0-1330-4754-a9ca-e791d6707b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.062 | validation accuracy:0.109\n",
      "For 500th epoch: train accuracy: 0.719 | validation accuracy:0.743\n",
      "For 1000th epoch: train accuracy: 0.789 | validation accuracy:0.749\n",
      "For 1500th epoch: train accuracy: 0.688 | validation accuracy:0.756\n",
      "For 2000th epoch: train accuracy: 0.766 | validation accuracy:0.757\n",
      "For 2500th epoch: train accuracy: 0.695 | validation accuracy:0.757\n",
      "For 3000th epoch: train accuracy: 0.758 | validation accuracy:0.750\n",
      "For 3500th epoch: train accuracy: 0.719 | validation accuracy:0.741\n",
      "For 4000th epoch: train accuracy: 0.781 | validation accuracy:0.727\n",
      "For 4500th epoch: train accuracy: 0.750 | validation accuracy:0.712\n",
      "For 5000th epoch: train accuracy: 0.641 | validation accuracy:0.697\n",
      "For 5500th epoch: train accuracy: 0.617 | validation accuracy:0.681\n",
      "For 6000th epoch: train accuracy: 0.648 | validation accuracy:0.668\n",
      "For 6500th epoch: train accuracy: 0.695 | validation accuracy:0.656\n",
      "For 7000th epoch: train accuracy: 0.617 | validation accuracy:0.645\n",
      "For 7500th epoch: train accuracy: 0.633 | validation accuracy:0.631\n",
      "For 8000th epoch: train accuracy: 0.609 | validation accuracy:0.620\n",
      "For 8500th epoch: train accuracy: 0.609 | validation accuracy:0.613\n",
      "For 9000th epoch: train accuracy: 0.516 | validation accuracy:0.605\n",
      "For 9500th epoch: train accuracy: 0.477 | validation accuracy:0.593\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q3)\n",
    "l2 = init(q3, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ace48425-fc98-4baa-9d02-44a8b8793e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 31.17%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50eb8de-73c8-4986-a7d9-e8868d715c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1a3ba1-76f7-4800-b370-ca70b1a04a18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multi Layer Perceptron + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1ccce428-feb4-4cfe-a003-13daf2de8078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da raiz quadrada:  88\n"
     ]
    }
   ],
   "source": [
    "# Regra da raiz quadrada\n",
    "q2 = np.sqrt(p * m)\n",
    "q2 = int(q2)\n",
    "print('Regra da raiz quadrada: ', q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3bfef4e0-97b5-435c-9a39-3a6cdf8c03aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests, gzip, os, hashlib\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "(X, Y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Validation split\n",
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val=X[train_no,:,:],X[val_no,:,:]\n",
    "y_train,y_val=Y[train_no],Y[val_no]\n",
    "\n",
    "X_train = X_train.reshape((-1, 28*28))\n",
    "X_val = X_val.reshape((-1, 28*28))\n",
    "X_test = X_test.reshape((-1, 28*28))\n",
    "\n",
    "def init(x, y):\n",
    "    layer = np.random.uniform(-1, 1., size=(x,y)) / np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1)\n",
    "\n",
    "# derivative of sigmoid\n",
    "def d_sigmoid(x):\n",
    "    return (np.exp(-x)) / ((np.exp(-x) + 1)**2)\n",
    "\n",
    "# sofmax function\n",
    "def softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0)\n",
    "\n",
    "# derivative of softmax\n",
    "def d_softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0) * (1-exp_element / np.sum(exp_element, axis=0))\n",
    "\n",
    "# foward and backward pass\n",
    "def forward_backward_pass(x, y):\n",
    "    targets = np.zeros((len(y), 10), np.float32)\n",
    "    targets[range(targets.shape[0]), y] = 1\n",
    "\n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = softmax(x_l2)\n",
    "\n",
    "\n",
    "    error = 2 * (out - targets) / out.shape[0] * d_softmax(x_l2)\n",
    "    update_l2 = x_sigmoid.T @ error\n",
    "\n",
    "\n",
    "    error = ((l2).dot(error.T)).T * d_sigmoid(x_l1)\n",
    "    update_l1 = x.T @ error\n",
    "\n",
    "    return out, update_l1, update_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ade2dc31-4b0e-43b2-905f-0f21b5dc4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.109 | validation accuracy:0.086\n",
      "For 500th epoch: train accuracy: 0.148 | validation accuracy:0.196\n",
      "For 1000th epoch: train accuracy: 0.258 | validation accuracy:0.296\n",
      "For 1500th epoch: train accuracy: 0.422 | validation accuracy:0.381\n",
      "For 2000th epoch: train accuracy: 0.477 | validation accuracy:0.448\n",
      "For 2500th epoch: train accuracy: 0.578 | validation accuracy:0.499\n",
      "For 3000th epoch: train accuracy: 0.484 | validation accuracy:0.536\n",
      "For 3500th epoch: train accuracy: 0.562 | validation accuracy:0.561\n",
      "For 4000th epoch: train accuracy: 0.609 | validation accuracy:0.577\n",
      "For 4500th epoch: train accuracy: 0.586 | validation accuracy:0.596\n",
      "For 5000th epoch: train accuracy: 0.609 | validation accuracy:0.611\n",
      "For 5500th epoch: train accuracy: 0.562 | validation accuracy:0.625\n",
      "For 6000th epoch: train accuracy: 0.664 | validation accuracy:0.636\n",
      "For 6500th epoch: train accuracy: 0.664 | validation accuracy:0.643\n",
      "For 7000th epoch: train accuracy: 0.562 | validation accuracy:0.649\n",
      "For 7500th epoch: train accuracy: 0.641 | validation accuracy:0.654\n",
      "For 8000th epoch: train accuracy: 0.625 | validation accuracy:0.659\n",
      "For 8500th epoch: train accuracy: 0.688 | validation accuracy:0.663\n",
      "For 9000th epoch: train accuracy: 0.664 | validation accuracy:0.666\n",
      "For 9500th epoch: train accuracy: 0.688 | validation accuracy:0.669\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_val_pca = np.dot(X_val, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(num_components, q2)\n",
    "l2 = init(q2, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train_pca.shape[0], size=(batch))\n",
    "    x = X_train_pca[sample]\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val_pca.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f6913a87-dd81-43f4-b68b-75f715af038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 68.38%\n"
     ]
    }
   ],
   "source": [
    "test_out=np.argmax(softmax(sigmoid(X_test_pca.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23d5ba-bfe2-42c0-8c56-82c83bc014e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tentativa de código melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc89cb-3353-474f-980f-db333af0eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests, gzip, os, hashlib\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "class MLP():\n",
    "    def __init__(self, num_features, num_q, num_classes, learning_rate=0.001, num_epochs=10000, batch=128):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.num_q = num_q\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch = batch\n",
    "        \n",
    "    def arquitetura(self, x, y):\n",
    "        layer = np.random.uniform(-1, 1., size=(x,y)) / np.sqrt(x*y)\n",
    "        return layer.astype(np.float32)\n",
    "    \n",
    "    # sigmoid function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (np.exp(-x)+1)\n",
    "    \n",
    "    # derivative of sigmoid\n",
    "    def d_sigmoid(self, x):\n",
    "        return (np.exp(-x)) / ((np.exp(-x) + 1)**2)\n",
    "    \n",
    "    # sofmax function\n",
    "    def softmax(self, x):\n",
    "        exp_element = np.exp(x-x.max())\n",
    "        return exp_element / np.sum(exp_element, axis=0)\n",
    "    \n",
    "    # derivative of softmax\n",
    "    def d_softmax(self, x):\n",
    "        exp_element = np.exp(x-x.max())\n",
    "        return exp_element / np.sum(exp_element, axis=0) * (1-exp_element / np.sum(exp_element, axis=0))\n",
    "    \n",
    "    # foward and backward pass\n",
    "    def forward_backward_pass(self, x, y):\n",
    "        targets = np.zeros((len(y), self.num_classes), np.float32)\n",
    "        targets[range(targets.shape[0]), y] = 1\n",
    "    \n",
    "        x_l1 = x.dot(l1)\n",
    "        x_sigmoid = sigmoid(x_l1)\n",
    "    \n",
    "        x_l2 = x_sigmoid.dot(l2)\n",
    "        out = softmax(x_l2)\n",
    "    \n",
    "    \n",
    "        error = 2 * (out - targets) / out.shape[0] * d_softmax(x_l2)\n",
    "        update_l2 = x_sigmoid.T @ error\n",
    "    \n",
    "    \n",
    "        error = ((l2).dot(error.T)).T * d_sigmoid(x_l1)\n",
    "        update_l1 = x.T @ error\n",
    "    \n",
    "        return out, update_l1, update_l2\n",
    "\n",
    "    def predict(self, x):\n",
    "        X_test=X_test.reshape((-1, self.num_features))\n",
    "        return test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "        \n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        accuracies, losses, val_accuracies, losses_val, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "        for i in range(epochs):\n",
    "            sample = np.random.randint(0, X_train.shape[0], size=(self.batch))\n",
    "            x = X_train[sample].reshape((-1, self.num_features))\n",
    "            y = y_train[sample]\n",
    "        \n",
    "            out, update_l1, update_l2 = self.forward_backward_pass(x, y)\n",
    "        \n",
    "            category = np.argmax(out, axis=1)\n",
    "            accuracy = (category == y).mean()\n",
    "            accuracies.append(accuracy)\n",
    "        \n",
    "            loss = ((category - y)**2).mean()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            l1 = l1 - lr*update_l1\n",
    "            l2 = l2 - lr*update_l2\n",
    "        \n",
    "            if(i%20 == 0):\n",
    "                X_val = X_val.reshape((-1, 28*28))\n",
    "                val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "                val_acc = (val_out == y_val).mean()\n",
    "                val_accuracies.append(val_acc.item())\n",
    "                loss_val = ((val_out - y_val)**2).mean()\n",
    "                losses_val.append(loss_val.item())\n",
    "            if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')\n",
    "\n",
    "\n",
    "(X, Y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Validation split\n",
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val=X[train_no,:,:],X[val_no,:,:]\n",
    "y_train,y_val=Y[train_no],Y[val_no]\n",
    "\n",
    "\n",
    "\n",
    "# Obtenção do numero de neurônios ocultos\n",
    "# Regra da raiz quadrada\n",
    "q2 = np.sqrt(p * m)\n",
    "q2 = int(q2)\n",
    "print('Regra da raiz quadrada: ', q2)\n",
    "\n",
    "model = MLP(num_features=28*28, num_q=q2, num_classes=10)\n",
    "\n",
    "model.train(X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a149e1-c771-4847-9e92-086a71844346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67995926-5abc-4986-9e43-0bdb9cb0118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 17:29:19.663748: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-15 17:29:20.388933: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-15 17:29:20.393680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 17:29:22.239528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11839e32-8a8d-4830-9351-2a5e89c285fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81fce4c-705c-46b6-94dc-a1a15364ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34826 (136.04 KB)\n",
      "Trainable params: 34826 (136.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327f6c03-f29d-4571-be0c-2a4517bbf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 26s 59ms/step - loss: 0.3576 - accuracy: 0.8935 - val_loss: 0.0834 - val_accuracy: 0.9790\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 23s 54ms/step - loss: 0.1111 - accuracy: 0.9659 - val_loss: 0.0580 - val_accuracy: 0.9838\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 29s 68ms/step - loss: 0.0835 - accuracy: 0.9750 - val_loss: 0.0463 - val_accuracy: 0.9870\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 28s 67ms/step - loss: 0.0688 - accuracy: 0.9791 - val_loss: 0.0435 - val_accuracy: 0.9867\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 23s 55ms/step - loss: 0.0610 - accuracy: 0.9814 - val_loss: 0.0388 - val_accuracy: 0.9887\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 0.0562 - accuracy: 0.9825 - val_loss: 0.0374 - val_accuracy: 0.9888\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 27s 63ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 0.0344 - val_accuracy: 0.9902\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.0477 - accuracy: 0.9851 - val_loss: 0.0331 - val_accuracy: 0.9913\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 24s 56ms/step - loss: 0.0440 - accuracy: 0.9861 - val_loss: 0.0320 - val_accuracy: 0.9900\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 22s 53ms/step - loss: 0.0406 - accuracy: 0.9869 - val_loss: 0.0296 - val_accuracy: 0.9908\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0399 - accuracy: 0.9875 - val_loss: 0.0313 - val_accuracy: 0.9913\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0373 - accuracy: 0.9883 - val_loss: 0.0305 - val_accuracy: 0.9915\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.0304 - val_accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.0303 - val_accuracy: 0.9905\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 18s 43ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 0.0273 - val_accuracy: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff82445add0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1adff0d-e558-4c40-83fe-0649dba1ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.023813791573047638\n",
      "Test accuracy: 0.9923999905586243\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

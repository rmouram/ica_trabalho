{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ed15f0-45cf-4474-9f3b-d63ded6a23ca",
   "metadata": {},
   "source": [
    "# Questão 02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebefe1a-dbd0-4d92-a73f-0e6793658c9c",
   "metadata": {},
   "source": [
    "## Mínimos quadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3764081e-e809-4cd5-9184-f864c44c9a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro médio quadrado (MSE): 54.60475048894781\n",
      "Erro médio absoluto (MAE): 5.326757449309436\n",
      "R2: 0.6745062372078383\n",
      "Raiz do erro médio quadrado (RMSE): 7.389502722710629\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Read xlsx file\n",
    "ws = pd.read_excel(\"Real_estate_valuation_dataset.xlsx\", engine='openpyxl')\n",
    "# Drop useless column\n",
    "ws = ws.drop('No', axis=1)\n",
    "# Convert in numpy ndarray\n",
    "data = np.array(ws)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "def ridge_regression(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "def calculate_error(X, y, theta):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Realiza as predições\n",
    "    predictions = X @ theta\n",
    "\n",
    "    # Converte as predições em rótulos\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calcula o erro (taxa de erro)\n",
    "    error = np.mean(predicted_labels != y)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a regressão usando mínimos quadrados com regularização de Ridge\n",
    "theta = ridge_regression(x_train, y_train, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((x_test.shape[0], 1)), x_test), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Calcular o erro médio quadrado (MSE) nas previsões\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Erro médio quadrado (MSE):\", mse)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Erro médio absoluto (MAE):\", mae)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Raiz do erro médio quadrado (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7cf715-bc67-4b48-bf87-97de940f949b",
   "metadata": {},
   "source": [
    "## ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "195209a2-cb18-40a9-8643-bb0b8f86f75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 66.71673352060317\n",
      "Test MAE: 6.324687683775253\n",
      "R2: 0.6023078497681544\n",
      "Raiz do erro médio quadrado (RMSE): 8.168031189987166\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ELMRegressor:\n",
    "    def __init__(self, num_hidden_neurons):\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.weights_input_hidden = None\n",
    "        self.weights_hidden_output = None\n",
    "        self.bias_hidden = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Inicialização aleatória dos pesos e vieses\n",
    "        self.weights_input_hidden = np.random.rand(num_features, self.num_hidden_neurons)\n",
    "        self.bias_hidden = np.random.rand(1, self.num_hidden_neurons)\n",
    "\n",
    "        \n",
    "        # Calcula as saídas da camada oculta\n",
    "        hidden_output = self._relu(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
    "\n",
    "        # Calcula os pesos da camada de saída usando a pseudo-inversa\n",
    "        self.weights_hidden_output = np.dot(np.linalg.pinv(hidden_output), y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_output = self._relu(np.dot(X, self.weights_input_hidden) + self.bias_hidden)\n",
    "        predictions = np.dot(hidden_output, self.weights_hidden_output)\n",
    "        return predictions\n",
    "\n",
    "# Read xlsx file\n",
    "ws = pd.read_excel(\"Real_estate_valuation_dataset.xlsx\", engine='openpyxl')\n",
    "# Drop useless column\n",
    "ws = ws.drop('No', axis=1)\n",
    "# Convert in numpy ndarray\n",
    "data = np.array(ws)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "# Crie uma instância do regressor ELM e ajuste-o aos dados de treinamento\n",
    "elm_regressor = ELMRegressor(num_hidden_neurons=16)\n",
    "elm_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Faça previsões nos dados de teste\n",
    "predictions = elm_regressor.predict(X_test)\n",
    "#print(\"predictions: \", predictions)\n",
    "\n",
    "# Calcule o Mean Squared Error (MSE)\n",
    "mse = np.mean((y_test - predictions)**2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculando o MAE para o conjunto de testes\n",
    "mae_test = mean_absolute_error(y_test, predictions)\n",
    "print(\"Test MAE:\", mae_test)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Raiz do erro médio quadrado (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea2fdb-4bbe-4ca0-9a1d-f1b14d22ed0a",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df840d2e-33d3-4b6a-9b34-1c5449c7146a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1389.579348449164\n",
      "Epoch 100, Loss: 156.37180298715873\n",
      "Epoch 200, Loss: 285.4814588420636\n",
      "Epoch 300, Loss: 171.20383242004408\n",
      "Epoch 400, Loss: 94.50702702753408\n",
      "Epoch 500, Loss: 63.17298549792008\n",
      "Epoch 600, Loss: 107.94596233887168\n",
      "Epoch 700, Loss: 66.0014340836988\n",
      "Epoch 800, Loss: 92.68832780689397\n",
      "Epoch 900, Loss: 108.047838820616\n",
      "Epoch 1000, Loss: 93.92130956117094\n",
      "Epoch 1100, Loss: 50.605318658464704\n",
      "Epoch 1200, Loss: 64.22754526229127\n",
      "Epoch 1300, Loss: 97.25518668763925\n",
      "Epoch 1400, Loss: 74.19741776817122\n",
      "Epoch 1500, Loss: 45.17845496438835\n",
      "Epoch 1600, Loss: 48.48217341483631\n",
      "Epoch 1700, Loss: 204.25730517338718\n",
      "Epoch 1800, Loss: 71.20631976123488\n",
      "Epoch 1900, Loss: 35.65754939103971\n",
      "Epoch 2000, Loss: 68.41198639555947\n",
      "Epoch 2100, Loss: 75.4238776995586\n",
      "Epoch 2200, Loss: 67.4919752637213\n",
      "Epoch 2300, Loss: 131.2276928324167\n",
      "Epoch 2400, Loss: 85.7449322476584\n",
      "Epoch 2500, Loss: 427.7625645990054\n",
      "Epoch 2600, Loss: 93.42926277185893\n",
      "Epoch 2700, Loss: 42.82319532967115\n",
      "Epoch 2800, Loss: 77.13794645469014\n",
      "Epoch 2900, Loss: 46.315381809504814\n",
      "Epoch 3000, Loss: 95.05390521847573\n",
      "Epoch 3100, Loss: 116.92478177209742\n",
      "Epoch 3200, Loss: 63.377260683176104\n",
      "Epoch 3300, Loss: 40.61355881540547\n",
      "Epoch 3400, Loss: 63.084545921732506\n",
      "Epoch 3500, Loss: 41.70365355675282\n",
      "Epoch 3600, Loss: 79.89463844634524\n",
      "Epoch 3700, Loss: 78.78127142490514\n",
      "Epoch 3800, Loss: 38.06772689066596\n",
      "Epoch 3900, Loss: 75.28217113762821\n",
      "Epoch 4000, Loss: 43.73714761269695\n",
      "Epoch 4100, Loss: 65.45050029377401\n",
      "Epoch 4200, Loss: 80.49809160001651\n",
      "Epoch 4300, Loss: 67.17287632579604\n",
      "Epoch 4400, Loss: 53.752819649820424\n",
      "Epoch 4500, Loss: 59.09320690824924\n",
      "Epoch 4600, Loss: 71.42687450901055\n",
      "Epoch 4700, Loss: 58.74389207337801\n",
      "Epoch 4800, Loss: 56.16626917028789\n",
      "Epoch 4900, Loss: 45.78037171441569\n",
      "Epoch 5000, Loss: 114.79316841522166\n",
      "Epoch 5100, Loss: 46.20656239151266\n",
      "Epoch 5200, Loss: 28.41392731251092\n",
      "Epoch 5300, Loss: 233.7674088978549\n",
      "Epoch 5400, Loss: 33.13755653240868\n",
      "Epoch 5500, Loss: 58.578534983979225\n",
      "Epoch 5600, Loss: 68.41305438221264\n",
      "Epoch 5700, Loss: 40.3288696450465\n",
      "Epoch 5800, Loss: 82.40689349083513\n",
      "Epoch 5900, Loss: 195.72650167251413\n",
      "Epoch 6000, Loss: 77.21839086577428\n",
      "Epoch 6100, Loss: 53.7871672313323\n",
      "Epoch 6200, Loss: 42.25611317514709\n",
      "Epoch 6300, Loss: 45.11314705719063\n",
      "Epoch 6400, Loss: 92.20861708965587\n",
      "Epoch 6500, Loss: 49.358363925867444\n",
      "Epoch 6600, Loss: 57.426345839647354\n",
      "Epoch 6700, Loss: 102.45119913370726\n",
      "Epoch 6800, Loss: 30.97986757032104\n",
      "Epoch 6900, Loss: 36.63176046352966\n",
      "Epoch 7000, Loss: 57.89833553389356\n",
      "Epoch 7100, Loss: 25.970997343997198\n",
      "Epoch 7200, Loss: 32.21007927814631\n",
      "Epoch 7300, Loss: 44.63205505211495\n",
      "Epoch 7400, Loss: 39.1545024899646\n",
      "Epoch 7500, Loss: 83.429619393286\n",
      "Epoch 7600, Loss: 86.65939475665755\n",
      "Epoch 7700, Loss: 240.95765672379204\n",
      "Epoch 7800, Loss: 45.13165850347901\n",
      "Epoch 7900, Loss: 67.24932569508553\n",
      "Epoch 8000, Loss: 30.930620007668143\n",
      "Epoch 8100, Loss: 81.84491409558083\n",
      "Epoch 8200, Loss: 69.98495859418432\n",
      "Epoch 8300, Loss: 38.68832476569935\n",
      "Epoch 8400, Loss: 36.78689160088982\n",
      "Epoch 8500, Loss: 34.77090969699201\n",
      "Epoch 8600, Loss: 64.4165270702216\n",
      "Epoch 8700, Loss: 76.86037045016502\n",
      "Epoch 8800, Loss: 33.26436685319797\n",
      "Epoch 8900, Loss: 67.05847153334878\n",
      "Epoch 9000, Loss: 114.19838676625773\n",
      "Epoch 9100, Loss: 84.16232074693612\n",
      "Epoch 9200, Loss: 30.61690461866757\n",
      "Epoch 9300, Loss: 97.18729246189105\n",
      "Epoch 9400, Loss: 45.05063976560486\n",
      "Epoch 9500, Loss: 31.964051914330458\n",
      "Epoch 9600, Loss: 23.869167334414016\n",
      "Epoch 9700, Loss: 49.30555281377014\n",
      "Epoch 9800, Loss: 52.87600511626179\n",
      "Epoch 9900, Loss: 53.532179414247\n",
      "Epoch 10000, Loss: 74.11701977130532\n",
      "Epoch 10100, Loss: 42.56708448031935\n",
      "Epoch 10200, Loss: 78.39845595298513\n",
      "Epoch 10300, Loss: 61.63899670168185\n",
      "Epoch 10400, Loss: 41.55926425428797\n",
      "Epoch 10500, Loss: 50.36928539849575\n",
      "Epoch 10600, Loss: 33.71664675353229\n",
      "Epoch 10700, Loss: 74.32150074484397\n",
      "Epoch 10800, Loss: 32.92593583932869\n",
      "Epoch 10900, Loss: 259.7239757020011\n",
      "Epoch 11000, Loss: 50.8577985358165\n",
      "Epoch 11100, Loss: 100.34675114963485\n",
      "Epoch 11200, Loss: 36.23836698638939\n",
      "Epoch 11300, Loss: 119.61554097552681\n",
      "Epoch 11400, Loss: 104.66637026196385\n",
      "Epoch 11500, Loss: 27.48743286796818\n",
      "Epoch 11600, Loss: 38.982028940404575\n",
      "Epoch 11700, Loss: 77.98205371204463\n",
      "Epoch 11800, Loss: 33.011270090275005\n",
      "Epoch 11900, Loss: 30.060506720756607\n",
      "Epoch 12000, Loss: 58.02399478792202\n",
      "Epoch 12100, Loss: 60.29072336026293\n",
      "Epoch 12200, Loss: 63.02943879362924\n",
      "Epoch 12300, Loss: 79.37662419014302\n",
      "Epoch 12400, Loss: 127.97530445066576\n",
      "Epoch 12500, Loss: 57.65673964713143\n",
      "Epoch 12600, Loss: 20.364469666604236\n",
      "Epoch 12700, Loss: 121.62288291462849\n",
      "Epoch 12800, Loss: 96.3881699463467\n",
      "Epoch 12900, Loss: 96.14851098196021\n",
      "Epoch 13000, Loss: 79.35561300819774\n",
      "Epoch 13100, Loss: 34.745520107827346\n",
      "Epoch 13200, Loss: 38.36757179318006\n",
      "Epoch 13300, Loss: 61.90053357356763\n",
      "Epoch 13400, Loss: 65.47428583707341\n",
      "Epoch 13500, Loss: 48.314837161183476\n",
      "Epoch 13600, Loss: 189.25940138548236\n",
      "Epoch 13700, Loss: 46.097438446113564\n",
      "Epoch 13800, Loss: 285.6724007972526\n",
      "Epoch 13900, Loss: 94.42691141696247\n",
      "Epoch 14000, Loss: 35.17541651065679\n",
      "Epoch 14100, Loss: 44.273850379243356\n",
      "Epoch 14200, Loss: 46.084989720955456\n",
      "Epoch 14300, Loss: 206.91526741590164\n",
      "Epoch 14400, Loss: 29.83625914901536\n",
      "Epoch 14500, Loss: 31.239158925478534\n",
      "Epoch 14600, Loss: 61.2021634961089\n",
      "Epoch 14700, Loss: 56.040548016412814\n",
      "Epoch 14800, Loss: 35.836713716753586\n",
      "Epoch 14900, Loss: 32.2481004769663\n",
      "Epoch 15000, Loss: 57.795571202267745\n",
      "Epoch 15100, Loss: 51.375961552691656\n",
      "Epoch 15200, Loss: 36.45945319605412\n",
      "Epoch 15300, Loss: 25.151744696542686\n",
      "Epoch 15400, Loss: 186.5447314704253\n",
      "Epoch 15500, Loss: 78.75812122739232\n",
      "Epoch 15600, Loss: 50.06169893603785\n",
      "Epoch 15700, Loss: 52.16126555228946\n",
      "Epoch 15800, Loss: 47.65993506906693\n",
      "Epoch 15900, Loss: 65.34956687770728\n",
      "Epoch 16000, Loss: 51.12882808625242\n",
      "Epoch 16100, Loss: 212.61287493489468\n",
      "Epoch 16200, Loss: 41.78888739818754\n",
      "Epoch 16300, Loss: 22.316652009560848\n",
      "Epoch 16400, Loss: 95.81247128936621\n",
      "Epoch 16500, Loss: 71.46805320793317\n",
      "Epoch 16600, Loss: 32.1543291916684\n",
      "Epoch 16700, Loss: 81.02730462418815\n",
      "Epoch 16800, Loss: 32.82111718211583\n",
      "Epoch 16900, Loss: 51.789393427969415\n",
      "Epoch 17000, Loss: 33.36594071270694\n",
      "Epoch 17100, Loss: 92.45717743207338\n",
      "Epoch 17200, Loss: 29.87902569329603\n",
      "Epoch 17300, Loss: 99.1247501927795\n",
      "Epoch 17400, Loss: 68.47779188541902\n",
      "Epoch 17500, Loss: 85.058474623445\n",
      "Epoch 17600, Loss: 64.52560750591714\n",
      "Epoch 17700, Loss: 62.28242346798036\n",
      "Epoch 17800, Loss: 39.27549495604137\n",
      "Epoch 17900, Loss: 62.23219552832603\n",
      "Epoch 18000, Loss: 184.75454138597715\n",
      "Epoch 18100, Loss: 33.21407076592966\n",
      "Epoch 18200, Loss: 85.32862530586982\n",
      "Epoch 18300, Loss: 85.35868946304593\n",
      "Epoch 18400, Loss: 50.818004683311656\n",
      "Epoch 18500, Loss: 50.23444718534691\n",
      "Epoch 18600, Loss: 33.73039756178647\n",
      "Epoch 18700, Loss: 19.63549914075153\n",
      "Epoch 18800, Loss: 63.651557436890144\n",
      "Epoch 18900, Loss: 34.8663901810949\n",
      "Epoch 19000, Loss: 102.47070698715251\n",
      "Epoch 19100, Loss: 31.88779417343876\n",
      "Epoch 19200, Loss: 41.07582640556963\n",
      "Epoch 19300, Loss: 48.56807862632806\n",
      "Epoch 19400, Loss: 58.36746807109408\n",
      "Epoch 19500, Loss: 108.11101951124515\n",
      "Epoch 19600, Loss: 38.56633630182649\n",
      "Epoch 19700, Loss: 52.960434515291496\n",
      "Epoch 19800, Loss: 33.78321467861021\n",
      "Epoch 19900, Loss: 51.112953058480535\n",
      "Test MSE: 59.434262599412975\n",
      "Test MAE: 5.696686482822682\n",
      "R2: 0.6457179714395733\n",
      "Raiz do erro médio quadrado (RMSE): 7.709362009882074\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Read xlsx file\n",
    "ws = pd.read_excel(\"Real_estate_valuation_dataset.xlsx\", engine='openpyxl')\n",
    "# Drop useless column\n",
    "ws = ws.drop('No', axis=1)\n",
    "# Convert to numpy ndarray\n",
    "data = np.array(ws)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "def init(x, y):\n",
    "    layer = np.random.uniform(-1, 1., size=(x, y)) / np.sqrt(x * y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def forward_backward_pass(x, y):\n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = x_l2\n",
    "    \n",
    "    error = 2 * (out - y) / out.shape[0]\n",
    "    update_l2 = x_sigmoid.T @ error\n",
    "    \n",
    "    error = (error @ l2.T) * d_sigmoid(x_sigmoid)\n",
    "    update_l1 = x.T @ error\n",
    "    \n",
    "    return out, update_l1, update_l2\n",
    "\n",
    "epochs = 20000\n",
    "lr = 0.001\n",
    "batch = 32\n",
    "num_hidden_neurons = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(X_train.shape[1], num_hidden_neurons)\n",
    "l2 = init(num_hidden_neurons, 1)\n",
    "\n",
    "losses = []\n",
    "\n",
    "#y = y.reshape((-1, 1))\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample]\n",
    "    y = y_train[sample].reshape((-1, 1))\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    loss = mean_squared_error(y, out)  # Calculate MSE\n",
    "    losses.append(loss)\n",
    "\n",
    "    l1 -= lr * update_l1\n",
    "    l2 -= lr * update_l2\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}, Loss: {loss}')\n",
    "\n",
    "\n",
    "X_test = X_test.reshape((-1, X_train.shape[1]))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Teste\n",
    "def test(x, y, l1, l2):\n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = x_l2\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Calculando as previsões para o conjunto de testes\n",
    "y_pred_test = test(X_test, y_test, l1, l2)\n",
    "\n",
    "# Calculando o MSE para o conjunto de testes\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"Test MSE:\", mse_test)\n",
    "\n",
    "# Calculando o MAE para o conjunto de testes\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print(\"Test MAE:\", mae_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "rmse = np.sqrt(mse_test)\n",
    "print(\"Raiz do erro médio quadrado (RMSE):\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

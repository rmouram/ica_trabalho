{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb718b4-2376-4582-86cd-be41c37de427",
   "metadata": {},
   "source": [
    "# Minimos Quadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4d8e0d-9f14-4e6d-861d-513c9268535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 15:51:09.828776: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-20 15:51:10.347100: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-20 15:51:10.349509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 15:51:12.766681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "def calculate_error(X, y, theta):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Realiza as predições\n",
    "    predictions = X @ theta\n",
    "\n",
    "    # Converte as predições em rótulos\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calcula o erro (taxa de erro)\n",
    "    error = np.mean(predicted_labels != y)\n",
    "\n",
    "    return error\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71af66-3249-4d08-83e4-2d9461eac7fc",
   "metadata": {},
   "source": [
    "# Perceptron Logístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14754d2-4bbd-4309-9a10-fd281ca699a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6550bb67-5c48-423d-8766-4162dc1f837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)\n",
    "                error = target - probabilities\n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)\n",
    "\n",
    "# Treinar o perceptron\n",
    "perceptron.train(X_train, y_train)\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68398f-be04-41f0-abe1-decea7f82c5a",
   "metadata": {},
   "source": [
    "# Minimos Quadrados + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f61694d1-cf06-4006-8c2a-e439d25e72ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 200\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train_pca, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test_pca.shape[0], 1)), X_test_pca), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e16181-2f23-43b4-b3b2-cb66c4dc334a",
   "metadata": {},
   "source": [
    "# Perceptron Logistico + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e53df544-62e6-46ce-a08c-99e47885910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)\n",
    "                error = target - probabilities\n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta\n",
    "\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 200\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train_pca.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)\n",
    "\n",
    "# Treinar o perceptron\n",
    "perceptron.train(X_train_pca, y_train)\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test_pca:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d43cdd-c75e-4bcb-92de-d7f23bcd698e",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26ebd276-be88-4963-a255-1f106d27f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra do valor médio:  397\n",
      "Regra da raiz quadrada:  88\n",
      "Regra de kolmogorov:  1569\n"
     ]
    }
   ],
   "source": [
    "# Regras heurísticas para determinação do número de neurônios ocultos (q)\n",
    "# fonte referência: https://repositorio.ufc.br/bitstream/riufc/52214/3/2020_dis_juponte.pdf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "p = 28*28\n",
    "m = 10\n",
    "\n",
    "# Regra do valor médio\n",
    "q1 = (p + m) / 2\n",
    "q1 = int(q1)\n",
    "print('Regra do valor médio: ', q1)\n",
    "\n",
    "# Regra da raiz quadrada\n",
    "q2 = np.sqrt(p * m)\n",
    "q2 = int(q2)\n",
    "print('Regra da raiz quadrada: ', q2)\n",
    "\n",
    "# Regra de Kolmogorov\n",
    "q3 = 2*p + 1\n",
    "q3 = int(q3)\n",
    "print('Regra de kolmogorov: ', q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "634447af-479a-44b8-94d3-89350dfd455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests, gzip, os, hashlib\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "(X, Y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Validation split\n",
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val=X[train_no,:,:],X[val_no,:,:]\n",
    "y_train,y_val=Y[train_no],Y[val_no]\n",
    "\n",
    "def init(x, y):\n",
    "    layer = np.random.uniform(-1, 1., size=(x,y)) / np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1)\n",
    "\n",
    "# derivative of sigmoid\n",
    "def d_sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1) * (1 - 1 / (np.exp(-x)+1))\n",
    "# def d_sigmoid(x):\n",
    "#     return (np.exp(-x)) / ((np.exp(-x) + 1)**2)\n",
    "\n",
    "# sofmax function\n",
    "def softmax(x):\n",
    "    exp_element = np.exp(x-np.max(x))\n",
    "    return exp_element / np.sum(exp_element, axis=0)\n",
    "\n",
    "# derivative of softmax\n",
    "def d_softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0) * (1-exp_element / np.sum(exp_element, axis=0))\n",
    "\n",
    "\n",
    "# foward and backward pass\n",
    "def forward_backward_pass(x, y):\n",
    "    targets = np.zeros((len(y), 10), np.float32)\n",
    "    targets[range(targets.shape[0]), y] = 1    \n",
    "    \n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = softmax(x_l2)\n",
    "\n",
    "    error = 2 * (out - targets) / out.shape[0] * d_softmax(x_l2)\n",
    "    update_l2 = x_sigmoid.T @ error\n",
    "\n",
    "    error = ((l2).dot(error.T)).T * d_sigmoid(x_l1)\n",
    "    update_l1 = x.T @ error\n",
    "\n",
    "    return out, update_l1, update_l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beb7f6-4b07-4f6b-a798-15338d2c54ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regra 1: Valor médio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e9bd586-eae1-49c0-aff3-2cc56820b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.062 | validation accuracy:0.078\n",
      "For 500th epoch: train accuracy: 0.727 | validation accuracy:0.701\n",
      "For 1000th epoch: train accuracy: 0.711 | validation accuracy:0.730\n",
      "For 1500th epoch: train accuracy: 0.773 | validation accuracy:0.745\n",
      "For 2000th epoch: train accuracy: 0.773 | validation accuracy:0.755\n",
      "For 2500th epoch: train accuracy: 0.844 | validation accuracy:0.763\n",
      "For 3000th epoch: train accuracy: 0.859 | validation accuracy:0.769\n",
      "For 3500th epoch: train accuracy: 0.820 | validation accuracy:0.772\n",
      "For 4000th epoch: train accuracy: 0.805 | validation accuracy:0.774\n",
      "For 4500th epoch: train accuracy: 0.703 | validation accuracy:0.776\n",
      "For 5000th epoch: train accuracy: 0.797 | validation accuracy:0.776\n",
      "For 5500th epoch: train accuracy: 0.773 | validation accuracy:0.778\n",
      "For 6000th epoch: train accuracy: 0.781 | validation accuracy:0.776\n",
      "For 6500th epoch: train accuracy: 0.789 | validation accuracy:0.776\n",
      "For 7000th epoch: train accuracy: 0.820 | validation accuracy:0.775\n",
      "For 7500th epoch: train accuracy: 0.781 | validation accuracy:0.773\n",
      "For 8000th epoch: train accuracy: 0.852 | validation accuracy:0.770\n",
      "For 8500th epoch: train accuracy: 0.773 | validation accuracy:0.767\n",
      "For 9000th epoch: train accuracy: 0.727 | validation accuracy:0.762\n",
      "For 9500th epoch: train accuracy: 0.750 | validation accuracy:0.759\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q1)\n",
    "l2 = init(q1, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33a993dc-79ec-481d-a5d4-6727556cd9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 76.84%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6701e6e-7812-459d-9691-d40f7a08c30e",
   "metadata": {},
   "source": [
    "## Regra 2: Raiz quadrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee1deb18-5501-4dfb-b257-cd96ae9c3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.062 | validation accuracy:0.077\n",
      "For 500th epoch: train accuracy: 0.445 | validation accuracy:0.498\n",
      "For 1000th epoch: train accuracy: 0.711 | validation accuracy:0.604\n",
      "For 1500th epoch: train accuracy: 0.641 | validation accuracy:0.657\n",
      "For 2000th epoch: train accuracy: 0.656 | validation accuracy:0.684\n",
      "For 2500th epoch: train accuracy: 0.727 | validation accuracy:0.703\n",
      "For 3000th epoch: train accuracy: 0.734 | validation accuracy:0.720\n",
      "For 3500th epoch: train accuracy: 0.836 | validation accuracy:0.732\n",
      "For 4000th epoch: train accuracy: 0.812 | validation accuracy:0.743\n",
      "For 4500th epoch: train accuracy: 0.758 | validation accuracy:0.749\n",
      "For 5000th epoch: train accuracy: 0.719 | validation accuracy:0.756\n",
      "For 5500th epoch: train accuracy: 0.734 | validation accuracy:0.764\n",
      "For 6000th epoch: train accuracy: 0.750 | validation accuracy:0.768\n",
      "For 6500th epoch: train accuracy: 0.820 | validation accuracy:0.773\n",
      "For 7000th epoch: train accuracy: 0.727 | validation accuracy:0.777\n",
      "For 7500th epoch: train accuracy: 0.758 | validation accuracy:0.781\n",
      "For 8000th epoch: train accuracy: 0.734 | validation accuracy:0.782\n",
      "For 8500th epoch: train accuracy: 0.820 | validation accuracy:0.784\n",
      "For 9000th epoch: train accuracy: 0.758 | validation accuracy:0.787\n",
      "For 9500th epoch: train accuracy: 0.844 | validation accuracy:0.789\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q2)\n",
    "l2 = init(q2, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fd53e1f-5c5b-407a-bb87-98d2ee0c83a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 81.15%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26385606-fb63-44bb-ada8-002f02f577da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56a923c5-5a97-41cb-8f7a-914e58127876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regra 3: kolmogorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c902cbd0-1330-4754-a9ca-e791d6707b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.070 | validation accuracy:0.102\n",
      "For 500th epoch: train accuracy: 0.750 | validation accuracy:0.725\n",
      "For 1000th epoch: train accuracy: 0.672 | validation accuracy:0.733\n",
      "For 1500th epoch: train accuracy: 0.742 | validation accuracy:0.740\n",
      "For 2000th epoch: train accuracy: 0.727 | validation accuracy:0.741\n",
      "For 2500th epoch: train accuracy: 0.781 | validation accuracy:0.737\n",
      "For 3000th epoch: train accuracy: 0.664 | validation accuracy:0.734\n",
      "For 3500th epoch: train accuracy: 0.758 | validation accuracy:0.720\n",
      "For 4000th epoch: train accuracy: 0.773 | validation accuracy:0.709\n",
      "For 4500th epoch: train accuracy: 0.727 | validation accuracy:0.695\n",
      "For 5000th epoch: train accuracy: 0.680 | validation accuracy:0.680\n",
      "For 5500th epoch: train accuracy: 0.672 | validation accuracy:0.668\n",
      "For 6000th epoch: train accuracy: 0.633 | validation accuracy:0.654\n",
      "For 6500th epoch: train accuracy: 0.719 | validation accuracy:0.643\n",
      "For 7000th epoch: train accuracy: 0.594 | validation accuracy:0.631\n",
      "For 7500th epoch: train accuracy: 0.625 | validation accuracy:0.620\n",
      "For 8000th epoch: train accuracy: 0.562 | validation accuracy:0.610\n",
      "For 8500th epoch: train accuracy: 0.523 | validation accuracy:0.599\n",
      "For 9000th epoch: train accuracy: 0.539 | validation accuracy:0.589\n",
      "For 9500th epoch: train accuracy: 0.570 | validation accuracy:0.578\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(28*28, q3)\n",
    "l2 = init(q3, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train.shape[0], size=(batch))\n",
    "    x = X_train[sample].reshape((-1, 28*28))\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        X_val = X_val.reshape((-1, 28*28))\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ace48425-fc98-4baa-9d02-44a8b8793e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 57.05%\n"
     ]
    }
   ],
   "source": [
    "X_test=X_test.reshape((-1,28*28))\n",
    "test_out=np.argmax(softmax(sigmoid(X_test.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50eb8de-73c8-4986-a7d9-e8868d715c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1a3ba1-76f7-4800-b370-ca70b1a04a18",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ccce428-feb4-4cfe-a003-13daf2de8078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da raiz quadrada:  88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Regra da raiz quadrada\n",
    "q2 = np.sqrt(p * m)\n",
    "q2 = int(q2)\n",
    "print('Regra da raiz quadrada: ', q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bfef4e0-97b5-435c-9a39-3a6cdf8c03aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests, gzip, os, hashlib\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "(X, Y), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Validation split\n",
    "rand=np.arange(60000)\n",
    "np.random.shuffle(rand)\n",
    "train_no=rand[:50000]\n",
    "\n",
    "val_no=np.setdiff1d(rand,train_no)\n",
    "\n",
    "X_train,X_val=X[train_no,:,:],X[val_no,:,:]\n",
    "y_train,y_val=Y[train_no],Y[val_no]\n",
    "\n",
    "X_train = X_train.reshape((-1, 28*28))\n",
    "X_val = X_val.reshape((-1, 28*28))\n",
    "X_test = X_test.reshape((-1, 28*28))\n",
    "\n",
    "def init(x, y):\n",
    "    layer = np.random.uniform(-1, 1., size=(x,y)) / np.sqrt(x*y)\n",
    "    return layer.astype(np.float32)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1)\n",
    "\n",
    "# derivative of sigmoid\n",
    "def d_sigmoid(x):\n",
    "    return 1 / (np.exp(-x)+1) * (1 - (1 / (np.exp(-x)+1)))\n",
    "\n",
    "# sofmax function\n",
    "def softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0)\n",
    "\n",
    "# derivative of softmax\n",
    "def d_softmax(x):\n",
    "    exp_element = np.exp(x-x.max())\n",
    "    return exp_element / np.sum(exp_element, axis=0) * (1-exp_element / np.sum(exp_element, axis=0))\n",
    "\n",
    "# foward and backward pass\n",
    "def forward_backward_pass(x, y):\n",
    "    targets = np.zeros((len(y), 10), np.float32)\n",
    "    targets[range(targets.shape[0]), y] = 1\n",
    "\n",
    "    x_l1 = x.dot(l1)\n",
    "    x_sigmoid = sigmoid(x_l1)\n",
    "\n",
    "    x_l2 = x_sigmoid.dot(l2)\n",
    "    out = softmax(x_l2)\n",
    "\n",
    "\n",
    "    error = 2 * (out - targets) / out.shape[0] * d_softmax(x_l2)\n",
    "    update_l2 = x_sigmoid.T @ error\n",
    "\n",
    "\n",
    "    error = ((l2).dot(error.T)).T * d_sigmoid(x_l1)\n",
    "    update_l1 = x.T @ error\n",
    "\n",
    "    return out, update_l1, update_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ade2dc31-4b0e-43b2-905f-0f21b5dc4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0th epoch: train accuracy: 0.109 | validation accuracy:0.096\n",
      "For 500th epoch: train accuracy: 0.172 | validation accuracy:0.196\n",
      "For 1000th epoch: train accuracy: 0.281 | validation accuracy:0.317\n",
      "For 1500th epoch: train accuracy: 0.422 | validation accuracy:0.410\n",
      "For 2000th epoch: train accuracy: 0.461 | validation accuracy:0.473\n",
      "For 2500th epoch: train accuracy: 0.562 | validation accuracy:0.521\n",
      "For 3000th epoch: train accuracy: 0.484 | validation accuracy:0.554\n",
      "For 3500th epoch: train accuracy: 0.672 | validation accuracy:0.580\n",
      "For 4000th epoch: train accuracy: 0.664 | validation accuracy:0.603\n",
      "For 4500th epoch: train accuracy: 0.586 | validation accuracy:0.618\n",
      "For 5000th epoch: train accuracy: 0.680 | validation accuracy:0.630\n",
      "For 5500th epoch: train accuracy: 0.594 | validation accuracy:0.642\n",
      "For 6000th epoch: train accuracy: 0.719 | validation accuracy:0.650\n",
      "For 6500th epoch: train accuracy: 0.648 | validation accuracy:0.659\n",
      "For 7000th epoch: train accuracy: 0.656 | validation accuracy:0.666\n",
      "For 7500th epoch: train accuracy: 0.664 | validation accuracy:0.675\n",
      "For 8000th epoch: train accuracy: 0.609 | validation accuracy:0.679\n",
      "For 8500th epoch: train accuracy: 0.656 | validation accuracy:0.683\n",
      "For 9000th epoch: train accuracy: 0.641 | validation accuracy:0.689\n",
      "For 9500th epoch: train accuracy: 0.648 | validation accuracy:0.693\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_val_pca = np.dot(X_val, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "epochs = 10000\n",
    "lr = 0.001\n",
    "batch = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "l1 = init(num_components, q2)\n",
    "l2 = init(q2, 10)\n",
    "\n",
    "accuracies, losses, val_accuracies, val_losses, test_accuracies, test_losses = [], [], [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    sample = np.random.randint(0, X_train_pca.shape[0], size=(batch))\n",
    "    x = X_train_pca[sample]\n",
    "    y = y_train[sample]\n",
    "\n",
    "    out, update_l1, update_l2 = forward_backward_pass(x, y)\n",
    "\n",
    "    category = np.argmax(out, axis=1)\n",
    "    accuracy = (category == y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    loss = ((category - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    l1 = l1 - lr*update_l1\n",
    "    l2 = l2 - lr*update_l2\n",
    "\n",
    "    if(i%20 == 0):\n",
    "        val_out = np.argmax(softmax(sigmoid(X_val_pca.dot(l1)).dot(l2)), axis=1)\n",
    "        val_acc = (val_out == y_val).mean()\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        val_loss = ((val_out - y_val)**2).mean()\n",
    "        val_losses.append(val_loss.item())\n",
    "    if(i%500 == 0): print(f'For {i}th epoch: train accuracy: {accuracy:.3f} | validation accuracy:{val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6913a87-dd81-43f4-b68b-75f715af038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 70.77%\n"
     ]
    }
   ],
   "source": [
    "test_out=np.argmax(softmax(sigmoid(X_test_pca.dot(l1)).dot(l2)),axis=1)\n",
    "test_acc=(test_out==y_test).mean().item()\n",
    "print(f'Test accuracy = {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a149e1-c771-4847-9e92-086a71844346",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67995926-5abc-4986-9e43-0bdb9cb0118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11839e32-8a8d-4830-9351-2a5e89c285fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c81fce4c-705c-46b6-94dc-a1a15364ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34826 (136.04 KB)\n",
      "Trainable params: 34826 (136.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "327f6c03-f29d-4571-be0c-2a4517bbf9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 19:58:28.934116: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  3/422 [..............................] - ETA: 22s - loss: 2.2786 - accuracy: 0.1224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 19:58:30.132893: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17981568 exceeds 10% of free system memory.\n",
      "2023-08-20 19:58:30.133451: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17981568 exceeds 10% of free system memory.\n",
      "2023-08-20 19:58:30.185855: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17981568 exceeds 10% of free system memory.\n",
      "2023-08-20 19:58:30.186086: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17981568 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 23s 53ms/step - loss: 0.3554 - accuracy: 0.8917 - val_loss: 0.0794 - val_accuracy: 0.9788\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 20s 48ms/step - loss: 0.1118 - accuracy: 0.9658 - val_loss: 0.0600 - val_accuracy: 0.9835\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 0.0835 - accuracy: 0.9743 - val_loss: 0.0452 - val_accuracy: 0.9870\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0704 - accuracy: 0.9780 - val_loss: 0.0427 - val_accuracy: 0.9888\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.0609 - accuracy: 0.9810 - val_loss: 0.0404 - val_accuracy: 0.9890\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.0565 - accuracy: 0.9824 - val_loss: 0.0339 - val_accuracy: 0.9905\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 21s 51ms/step - loss: 0.0520 - accuracy: 0.9839 - val_loss: 0.0344 - val_accuracy: 0.9910\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 0.0483 - accuracy: 0.9852 - val_loss: 0.0304 - val_accuracy: 0.9920\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 23s 54ms/step - loss: 0.0457 - accuracy: 0.9855 - val_loss: 0.0329 - val_accuracy: 0.9918\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 30s 71ms/step - loss: 0.0426 - accuracy: 0.9865 - val_loss: 0.0312 - val_accuracy: 0.9925\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0389 - accuracy: 0.9874 - val_loss: 0.0288 - val_accuracy: 0.9925\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 19s 45ms/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 0.0302 - val_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 21s 49ms/step - loss: 0.0358 - accuracy: 0.9885 - val_loss: 0.0282 - val_accuracy: 0.9928\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 20s 47ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.0308 - val_accuracy: 0.9925\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 24s 56ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 0.0287 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1dd8f31d50>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1adff0d-e558-4c40-83fe-0649dba1ee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.025683002546429634\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

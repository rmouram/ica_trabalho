{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb718b4-2376-4582-86cd-be41c37de427",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Minimos Quadrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be4d8e0d-9f14-4e6d-861d-513c9268535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "def calculate_error(X, y, theta):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Realiza as predições\n",
    "    predictions = X @ theta\n",
    "\n",
    "    # Converte as predições em rótulos\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calcula o erro (taxa de erro)\n",
    "    error = np.mean(predicted_labels != y)\n",
    "\n",
    "    return error\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71af66-3249-4d08-83e4-2d9461eac7fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Perceptron Logístico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2677d38-1d40-48f7-b3ea-2a2cfd6bb524",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d58910a3-6ab3-4a8d-ac22-e317f54e6fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 88.05%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Cria uma instância do perceptron logístico\n",
    "model = Perceptron()\n",
    "\n",
    "# Treina o modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realiza as predições no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcula a acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Acurácia da classificação: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce720c1-b0d8-4d7a-8ecd-c69b4bf3376f",
   "metadata": {},
   "source": [
    "## A mão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14754d2-4bbd-4309-9a10-fd281ca699a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 21:10:33.742885: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 21:10:34.338036: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-14 21:10:34.340889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 21:10:35.836317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6550bb67-5c48-423d-8766-4162dc1f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=5):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]                \n",
    "                target = y[i]                \n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)              \n",
    "                error = target - activations                \n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f47de887-e728-479f-8d75-7cc8c13eacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f1077f8c-da7b-4923-a3be-950fef6e1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c37ffb7-1572-431d-80c2-a5d74f9089c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68e4d936-67fa-4096-8b98-11af26455203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16d57182-1cb0-4199-8523-9b1f906e0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o perceptron\n",
    "perceptron.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfcdb8d0-1f5f-4980-bad4-e95907d9a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b647280-98d3-4bfb-8f15-2e9bcdf6a5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7043\n"
     ]
    }
   ],
   "source": [
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68398f-be04-41f0-abe1-decea7f82c5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Minimos Quadrados + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f61694d1-cf06-4006-8c2a-e439d25e72ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 100\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train_pca, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test_pca.shape[0], 1)), X_test_pca), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00aab16-51f0-4b90-a52f-27244626dfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dc844-91eb-4abb-9c83-7beb088322d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac75ae3-1a38-4555-995f-7f6c9c779dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b226a62-be74-4724-9c5c-e03185f29fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a539259e-17ea-4031-9f18-60a95f51fd51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,) and (784,155) not aligned: 10000 (dim 0) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m X_train_pca \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_train, principal_components)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Projetar os dados de teste nas componentes principais\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m X_test_pca \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprincipal_components\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# -------------------------- PCA -------------------------- #\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------- #\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Converte os rótulos para uma representação one-hot\u001b[39;00m\n\u001b[1;32m     53\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10000,) and (784,155) not aligned: 10000 (dim 0) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def least_squares_classification(X, y, alpha):\n",
    "    # Adiciona uma coluna de 1s para representar o termo de viés\n",
    "    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "    # Calcula os coeficientes usando a fórmula dos mínimos quadrados com regularização de Ridge\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Carrega o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normaliza os dados dividindo por 255.0\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados de treino nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados de teste nas componentes principais\n",
    "X_test_pca = np.dot(y_test, principal_components)\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Converte os rótulos para uma representação one-hot\n",
    "num_classes = 10\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "# Define o valor de regularização (alpha)\n",
    "alpha = 0.01\n",
    "\n",
    "# Realiza a classificação usando mínimos quadrados com regularização de Ridge\n",
    "theta = least_squares_classification(X_train_pca, y_train_onehot, alpha)\n",
    "\n",
    "# Adiciona uma coluna de 1s aos dados de teste\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test_pca), axis=1)\n",
    "\n",
    "# Realiza as predições\n",
    "predictions = X_test @ theta\n",
    "\n",
    "# Converte as predições em rótulos\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# # Calcula a acurácia\n",
    "# accuracy = np.mean(predicted_labels == y_test) * 100\n",
    "# print(\"Acurácia da classificação: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e16181-2f23-43b4-b3b2-cb66c4dc334a",
   "metadata": {},
   "source": [
    "# Perceptron Logistico + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e53df544-62e6-46ce-a08c-99e47885910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class LogisticPerceptron:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, num_epochs=100):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = np.zeros((num_features + 1, num_classes))  # +1 for the bias term\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        activations = np.dot(np.insert(x, 0, 1), self.weights)\n",
    "        probabilities = self.sigmoid(activations)\n",
    "        return np.argmax(probabilities)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = np.insert(X, 0, 1, axis=1)  # Inserting bias term\n",
    "        y = np.eye(self.num_classes)[y]  # One-hot encoding\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                target = y[i]\n",
    "                activations = np.dot(x, self.weights)\n",
    "                probabilities = self.sigmoid(activations)\n",
    "                error = target - probabilities\n",
    "                delta = self.learning_rate * np.outer(x, error)\n",
    "                self.weights += delta\n",
    "\n",
    "\n",
    "# Carregando o conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Ajusta a forma dos dados\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalizar os dados\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "# Calcular a matriz de covariância\n",
    "cov_matrix = np.cov(X_train.T)\n",
    "\n",
    "# Calcular os autovetores e autovalores\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Ordenar os autovetores em ordem decrescente dos autovalores\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Escolher o número de componentes principais\n",
    "num_components = 155\n",
    "\n",
    "# Selecionar as componentes principais\n",
    "principal_components = sorted_eigenvectors[:, :num_components]\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_train_pca = np.dot(X_train, principal_components)\n",
    "\n",
    "# Projetar os dados nas componentes principais\n",
    "X_test_pca = np.dot(X_test, principal_components)\n",
    "\n",
    "\n",
    "# -------------------------- PCA -------------------------- #\n",
    "# --------------------------------------------------------- #\n",
    "\n",
    "# Criar o objeto do perceptron logístico\n",
    "num_features = X_train_pca.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "perceptron = LogisticPerceptron(num_features=num_features, num_classes=num_classes)\n",
    "\n",
    "# Treinar o perceptron\n",
    "perceptron.train(X_train_pca, y_train)\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "predictions = []\n",
    "for sample in X_test_pca:\n",
    "    prediction = perceptron.predict(sample)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calcular a precisão das previsões\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1be626-bb3b-442c-ac83-80da74f357be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MLP com Q neurônios ocultos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec29de1-da8c-4746-b4e6-cd6698543e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, MSE Loss: 0.1527\n",
      "Epoch 20/100, MSE Loss: 0.1388\n",
      "Epoch 30/100, MSE Loss: 0.1242\n",
      "Epoch 40/100, MSE Loss: 0.1116\n",
      "Epoch 50/100, MSE Loss: 0.1012\n",
      "Epoch 60/100, MSE Loss: 0.0926\n",
      "Epoch 70/100, MSE Loss: 0.0854\n",
      "Epoch 80/100, MSE Loss: 0.0796\n",
      "Epoch 90/100, MSE Loss: 0.0748\n",
      "Epoch 100/100, MSE Loss: 0.0708\n",
      "\n",
      "MSE nos dados de teste: 0.06976132299603907\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Inicialização dos pesos\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Propagação direta\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        # Retropropagação\n",
    "        m = X.shape[0]\n",
    "        self.dz2 = self.a2 - y\n",
    "        self.dW2 = (1/m) * np.dot(self.a1.T, self.dz2)\n",
    "        self.db2 = (1/m) * np.sum(self.dz2, axis=0, keepdims=True)\n",
    "        self.dz1 = np.dot(self.dz2, self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        self.dW1 = (1/m) * np.dot(X.T, self.dz1)\n",
    "        self.db1 = (1/m) * np.sum(self.dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Atualização dos pesos e viés\n",
    "        self.W1 -= learning_rate * self.dW1\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        self.W2 -= learning_rate * self.dW2\n",
    "        self.b2 -= learning_rate * self.db2\n",
    "    \n",
    "    def train(self, X, y, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Propagação direta e retropropagação\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            # Cálculo da função de custo (entropia cruzada categórica)\n",
    "            # loss = self.categorical_crossentropy(y_pred, y)\n",
    "            \n",
    "            # # Print do custo a cada 10 épocas\n",
    "            # if (epoch+1) % 10 == 0:\n",
    "            #     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Cálculo do MSE (Mean Squared Error)\n",
    "            mse_loss = self.mse(y_pred, y)\n",
    "\n",
    "            # Print do custo a cada 10 épocas\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, MSE Loss: {mse_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    def mse(self, y_pred, y_true):\n",
    "        return np.mean(np.square(y_pred - y_true))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def categorical_crossentropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        return -np.sum(y_true * np.log(y_pred)) / y_pred.shape[0]\n",
    "\n",
    "# Carregamento do conjunto de dados MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Criação do MLP\n",
    "mlp = MLP(input_size=784, hidden_size=512, output_size=10)\n",
    "\n",
    "# Treinamento do MLP\n",
    "mlp.train(X_train, y_train, num_epochs=100, learning_rate=0.1)\n",
    "\n",
    "# Avaliação do MLP nos dados de teste\n",
    "predictions = mlp.forward(X_test)\n",
    "# accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "# print(\"\\nAcurácia nos dados de teste:\", accuracy)\n",
    "mse = mlp.mse(predictions, y_test)\n",
    "print(\"\\nMSE nos dados de teste:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4a272-dfd0-4703-a43c-0d53859832ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
